<!DOCTYPE html>
<html lang="en">

<head><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<link rel="stylesheet" href="/assets/css/style.css">
<link rel="stylesheet" href="/assets/css/academicons-1.9.2/css/academicons.css">
<link rel="stylesheet" href="/assets/css/fontawesome-free-6.1.1-web/css/all.css">
<link href="https://fonts.googleapis.com/css?family=Merriweather:300|Raleway:400,700" rel="stylesheet">

<title>🌈 From-scratched Pose-graph SLAM Tutorial: 1편</title>

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>🌈 From-scratched Pose-graph SLAM Tutorial: 1편 | Giseop Kim Blog</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="🌈 From-scratched Pose-graph SLAM Tutorial: 1편" />
<meta name="author" content="Giseop Kim" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Single Rotation Averaging 을 구현해보자 어렵지 않아요 $\text{SO(3)}$ 와 $\mathfrak{so(3)}$ 미분 고수되는 법" />
<meta property="og:description" content="Single Rotation Averaging 을 구현해보자 어렵지 않아요 $\text{SO(3)}$ 와 $\mathfrak{so(3)}$ 미분 고수되는 법" />
<link rel="canonical" href="http://localhost:4000/slam/2024/08/24/rot-avg1.html" />
<meta property="og:url" content="http://localhost:4000/slam/2024/08/24/rot-avg1.html" />
<meta property="og:site_name" content="Giseop Kim Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-08-24T10:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="🌈 From-scratched Pose-graph SLAM Tutorial: 1편" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Giseop Kim"},"dateModified":"2024-08-24T10:00:00+09:00","datePublished":"2024-08-24T10:00:00+09:00","description":"Single Rotation Averaging 을 구현해보자 어렵지 않아요 $\\text{SO(3)}$ 와 $\\mathfrak{so(3)}$ 미분 고수되는 법","headline":"🌈 From-scratched Pose-graph SLAM Tutorial: 1편","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/slam/2024/08/24/rot-avg1.html"},"url":"http://localhost:4000/slam/2024/08/24/rot-avg1.html"}</script>
<!-- End Jekyll SEO tag -->


<script type="text/javascript" src="/assets/js/darkmode.js"></script>


<!-- fabicon -->
<link rel="icon" type="image/png" href="/assets/icon/me.png">
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$'] ],
        processEscapes: true,
      }
    });
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
</script>
<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
</head><body>
  <main class="container">
    <section class="about">
      <div class="about-header condensed">
      <div class="about-title">
      <a href="/">
        
        <img src="/assets/giseopkim_thermal.png" alt="Giseop Kim" />
        
      </a>
      <h2 id="title">
        <a href="/">Giseop Kim</a>
      </h2>
      </div><p class="tagline">SLAM Engineer</p></div>
      
      <ul class="social about-footer condensed"><a href="https://www.linkedin.com/in/giseop-kim-71683088" target="_blank">
          <li>
            <i class="icon-linkedin-squared"></i>
          </li>
        </a><a href="https://scholar.google.com/citations?user=9mKOLX8AAAAJ&hl=ko&oi=ao" target="_blank">
          <li>
            <i class="ai ai-google-scholar"></i>
          </li>
        </a><a href="https://github.com/gisbi-kim" target="_blank">
          <li>
            <i class="icon-github-circled"></i>
          </li>
        </a><a href="https://bit.ly/giseopkim" target="_blank">
          <li>
            <i class="fa fa-user" style="font-size: 2.09em;"></i>
            <!-- <i> notion</i> -->
            <!-- <i class="fa fa-user"></i> -->
          </li>
        </a><a href="https://youtube.com/channel/UCrmVMJ3KEFbDD9EtnAmDT6g" target="_blank">
          <li>
            <i class="icon-youtube"></i>
          </li>
        </a></ul><p class="about-footer condensed">&copy;
        2024</p><div class="about-footer condensed">
        <p>Dark Mode
          <i class="icon-moon"></i>
          <label class="switch">
            <input type="checkbox" class="dark-mode-toggle">
            <span class="slider round" onclick="toggleDarkMode()"></span>
          </label>
        </p>
      </div>
    </section>
    <section class="content">
      <div class="post-container">
  <a class="post-link" href="/slam/2024/08/24/rot-avg1.html">
    <h2 class="post-title">🌈 From-scratched Pose-graph SLAM Tutorial: 1편</h2>
  </a>
  <hr style="border:1px foo rgb(193, 198, 200)">
  <div class="post-meta">
    <div class="post-date"><i class="icon-calendar"></i>Aug 24, 2024</div><ul class="post-categories"><li>SLAM</li></ul></div>
  <div class="post">
    <h1 id="single-rotation-averaging-을-구현해보자">Single Rotation Averaging 을 구현해보자</h1>
<ul>
  <li>어렵지 않아요</li>
  <li>$\text{SO(3)}$ 와 $\mathfrak{so(3)}$ 미분 고수되는 법</li>
</ul>

<h2 id="개요">개요</h2>
<ul>
  <li>Rotation averaging 은 SfM (Structure-from-motion), SLAM 등에서 자주 등장하는 task 이다.
    <ul>
      <li>ps. rotation averaging 이 무엇인지에 대해서는 다음 논문을 참고.
        <ul>
          <li><a href="https://users.cecs.anu.edu.au/~hartley/Papers/PDF/Hartley-Trumpf:Rotation-averaging:IJCV.pdf">Rotation Averaging by Richard Hartley, et al.</a></li>
          <li>이 논문을 인용한 논문들 중 인용수가 많은 것을 보는 것 또한 도움이 되겠다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>왜 필요한가?
    <ul>
      <li>대부분의 geometric computer vision 문제에서, residual 을 최소화 할 때,</li>
      <li>현재 solution으로부터 delta 만큼 조금씩 이동해나가면서 최종 solution 으로 수렴해나가는데 (i.e., Gauss-Newton),</li>
      <li>이 작업은 vector space 에서만 가능하므로,</li>
      <li>nonlinear 한 term 들을 국소적으로 linearization 해주어야 하는데,
        <ul>
          <li>이 이야기에 대한 더 자세한 설명은 <a href="/slam/2023/12/17/nano-lie-theory.html">예전 포스트</a> 를 참고.</li>
        </ul>
      </li>
      <li>이 때, 직전 solution 이 안 좋다면 (== global solution 으로부터 멀다면)</li>
      <li>해가 수렴하기 어렵다.</li>
      <li>이것이 SfM or SLAM 에서 더욱 문제가 되는 건,</li>
      <li>N 개의 노드 (수십개일수도, 수천 수만 개 일수도) 의 pose 를 모두 joint 하게 최적화 하는 문제를 푸는 것이 SfM or SLAM 인데</li>
      <li>rotation 이 nonlinear 한 요소이고,</li>
      <li>Gauss-newton 과정에서, rotation 부분의 current solution 이 안좋다면 이는 pose 중 rotation 이 아니라 position 부분에도 악영향을 끼칠 수 있기 때문이다 (그리고 마찬가지로 그러한 한 노드의 에러가 다른 노드들에도 악영향을 끼칠 수도 있겠다).</li>
    </ul>
  </li>
  <li>그래서, 특히 nonlinear 한 term인 rotation 부분을 global solution에 잘 맞게 초기값을 잘 설정해둘 수 있다면, translation loss 등은 또한 쉽게 풀릴 수 있을 것이다.</li>
  <li>예를 들어서, 하나의 노드 (pose) 의 rotation (attitude) 에 대한 measurement 가 k 개 있을 때, 이것을 수만개 노드의 joint optimization 을 풀 때 모두 constraint 로 넣어주는 것이 아니라, k 개의 measurement 들을 잘 분석해서 신뢰할만한 하나의 값만을 도출한 다음, 그것만을 전체 최적화 문제의 constraint 로 추가해줄 수 있겠다.</li>
  <li>즉, <strong>single rotation averaging</strong> 이라는 task는,
    <ul>
      <li><strong>하나</strong>의 노드 (pose) 의 rotation (attitude) 에 대한 measurement 가 k 개 있을 때,</li>
      <li>이 측정치들을 모두 적당히 만족하도록 고려했을 때,</li>
      <li>그 노드의 rotation (attitude or orientation 등으로도 불림) 은 그래서 최종적으로 무엇인가, 에 대한 대답을 하는 문제이다.</li>
    </ul>
  </li>
  <li>rotation (i.e., SO(3)) 은 vector space 에 살고 있지 않기 때문에, 단순히 xyz 들을 k 로 나누는, 즉, $ \textbf{sum}(\textbf{R}_1, \textbf{R}_2, \textbf{R}_3) / 3$ 과 같이 나이브하게 계산할 수는 없다.</li>
  <li>물론 sum 이라는 함수와 / 라는 나눗셈을 어떻게 정의할거냐에 달린것이긴 하지만… 여기에서 이야기하려고 하는 것은 저런 deterministic 한 계산을 한 방에 하려는 것이 아니라, 대신에 iteratively 조정해나가는 과정을 어떻게 수행할 수 있느냐? 하는 점이다. 즉, 저 평균조차도 iterative 하게 얻어져야 한다는 것이다. 이는 다시 말하지만 rotation 이 살고 있는 공간은 nonlinear 하고 국소적으로 (== iterative optimization 이 수행되는 tangent space) 만 vector space 이기 때문에 저런 일반 연산을 수행할 수 있기 때문이다. 
<!-- - 즉, rotation averaging 은 번역체 그대로 이야기하면 좀 어색할 수도 있겠다. 그것의 의미는 평균이기는 하지만, 실제로 그것이 얻어지는 numerically mechanical 한 과정은 최적화이기 때문이다. 측정치들을 모두 만족하는 solution을 (최적화를 통해) 구하는 일이라고 해석해야 할 것이다.  --></li>
  <li>위 논문 <a href="https://users.cecs.anu.edu.au/~hartley/Papers/PDF/Hartley-Trumpf:Rotation-averaging:IJCV.pdf">Rotation Averaging by Richard Hartley, et al.</a> 에서는 rotation definition을 먼저 하고, 유용한 성질들에 대해 이야기 한 뒤, 여러 distance 에 대해서도 논의하고, 메인에서는 1. single rotation averaging 과 2. multiple rotation averaging 에 대해 이야기한다.</li>
  <li>그 중, 이 포스트에서는 1. single rotation averaging  에 대해서만 알아볼 것이다.
    <ul>
      <li>rotation 의 정의나 기본적인 성질에 대해서는 독자가 알고있다고 가정한다.</li>
      <li>그나저나 2. multiple rotation averaging 는 rotation only pose-graph optimization 이라고도 불린다.</li>
    </ul>
  </li>
  <li>그리고 위 논문을 보면 rotation 이 살고있는 공간에서 distance 를 정의하는 방식이 여러 개가 있음을 알 수 있는데, 본 포스트에서는 가장 기본적이고 정석적인 tangent space 에서의 L2 loss 로 정의할 것이다. 그리고 그 차이를 iteratively 줄여나가면, 원래 rotation 공간 (we call it manifold, SO(3)) 에서도 최적화가 되어있는지 보일 것이다.</li>
</ul>

<h2 id="티저">티저</h2>
<ul>
  <li>이 포스트에서는, single rotation averaging 을 from scratch 로 구현해보고 실습해보자.</li>
  <li>티저
    <ul>
      <li>결과부터 먼저 보이자면 아래 그림과 같다.
        <ul>
          <li>설명
            <ul>
              <li>빨강, 청록, 노랑 axes 들은, 어떤 로봇의 어떤 시점에서의 대한 자세 (rotation, attitude, orientation 등으로 불림) 예측 값을 의미한다.
                <ul>
                  <li>예를 들어서 동일한 센서로 세 번 측정했는데, 현실세계의 센서란 noisy 하고 불완전하므로 조금씩 저렇게 다른 결과가 나올 수 있겠다.</li>
                </ul>
              </li>
              <li>그러면 저 셋 정보를 모두 만족하는 (== 개념적으로 평균, 실제 계산은 iterative optimization) 값을 구해서, 로봇의 자세는 이것이야! 라고 말해야 할텐데,</li>
              <li>그 결과는 파랑색 축이다.</li>
              <li>일단 사람이 시각적으로 보았을 때 말이 맞는 것 같아보인다. 파랑이 세 축 모두에서, 다른 셋 들의 중간 즈음에 위치하는 것을 잘 볼 수 있다.</li>
            </ul>
          </li>
          <li>어떻게 구현했는지 이제 차차 설명을 해보자.</li>
        </ul>
      </li>
    </ul>
    <figure id="onmanifold_gaussnewton_result" align="center">
      <img src="/assets/data/2024-08-24-rot-avg1/main_onmanifold_ganussnewton_result.png" width="800" />
      <figcaption>On-manifold Gauss-Newton Result</figcaption>
  </figure>
  </li>
</ul>

<h2 id="배경지식-1">배경지식 1</h2>
<ul>
  <li>일단 iterative optimization 의 기계적인 과정은 아래와 같다 (이걸 나는 개인적으로 ‘기계적인 과정’ 이라던가, ‘산수’ 라던가, ‘계산기’라고 부른다. 즉, 어렵지 않다는 말이다).
    <ul>
      <li>이 그림은 매번 추천하는 자료인, <a href="http://www.diag.uniroma1.it/~labrococo/tutorial_icra_2016/">ICRA 2016 SLAM tutorial</a> 중, <a href="http://www.diag.uniroma1.it/~labrococo/tutorial_icra_2016/icra16_slam_tutorial_grisetti.pdf">Giorgio Grisetti 교수님 슬라이드</a> 에서 가져온 것이다.</li>
    </ul>
    <figure id="gn_manifold" align="center">
      <img src="/assets/data/2024-08-24-rot-avg1/gn_manifold.png" width="600" />
      <figcaption>GN Manifold Diagram</figcaption>
  </figure>
  </li>
  <li>여기서 $\textbf{J}$ 만 구하면 된다.
    <ul>
      <li>ps. 내가 다루는 1. 센서 특성 (예: 카메라는 pixel 로 구성되어있고 세상의 정보가 projective 하게 생성된다) 과, 2. 그 센서 특성을 고려한 observation model (residual model) (예: 그러므로 pose 가 true였다면 projection 했을 때의 reprojection error 가 작을 것이다. 그것은 (u, v) 공간에서의 그저 두 차원 벡터의 뺄셈으로 정의된다), 이 두 가지를 어떻게 잘 알고 잘 다루고 잘 구현할 수 있느냐가 단순히 geometric vision 전공자를 넘어서서 SLAM engineer의 도메인 놀리지라고 할 수 있겠다.</li>
      <li>J가 어떻게 구해지느냐만 다르지, 그 이후과정은 모든 논문들이 동일하다.</li>
    </ul>
  </li>
</ul>

<h2 id="배경지식-2">배경지식 2</h2>
<ul>
  <li>$\textbf{J}$ …</li>
  <li>자코비안, 야코비안, … 다양하게 불리는 이 녀석
    <ul>
      <li>그냥 multi-dimensional 기울기라고 생각하면 된다.</li>
      <li>Gauss-Newton optimization 을 하기 위해서는 기울기 값을 알아야 하기 때문.</li>
      <li><a href="/slam/2024/05/25/reprojection-jacobian-r2live.html">예전 포스트</a> 에서도 이 얘기는 이미 했었다 (사실 결국 맨날 이 얘기를 하게 되는 것이다 …).
        <ul>
          <li>여기서도 다시 말하자면, 자코비안은 이렇게 기억하면 된다. matrix 네모의 세로(즉, row 들) 는 cost, 가로(즉, column들)는 state.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>아무튼 본 포스트에서도 결국, 어떤 residual function에 대한, 최적 rotation 의 미소변화량, 의 미분 matrix (== 자코비안) 를 계산하는 것이 관건이 된다.</li>
  <li>rotation 에 대해 해보자.</li>
</ul>

<h2 id="유도-과정-derivation">유도 과정 (Derivation)</h2>
<ul>
  <li>iterative 하게 an averaged rotation 값을 예측할 것이므로,</li>
  <li>현재 ($t$ 번째 step) 까지 예측된 값을 $\textbf{R}^{(t)}$ 이라고 해보자.
    <ul>
      <li>== <a href="#onmanifold_gaussnewton_result">위의 티저 그림</a>에서 파랑 화살표 (혹은 이것에 도달하기 전의 스텝)</li>
    </ul>
  </li>
  <li>그리고 어떤 measurement $\textbf{R}_1$, $\textbf{R}_2$, $\textbf{R}_3$ 이렇게 3개가 있었다고 하자.
    <ul>
      <li>== <a href="#onmanifold_gaussnewton_result">위의 티저 그림</a>에서 파랑을 제외한 나머지 화살표들</li>
    </ul>
  </li>
  <li>그러면, 이 세 개의 measurement 로부터 각각 $\textbf{H}_i$ 와 $\textbf{e}_i$ 가 생성되고,</li>
  <li>전체 $\textbf{H}$ 와 $\textbf{b}$ 는 <a href="#gn_manifold">앞서 소개한 tutorial slide</a> 에서 설명한대로 (단순한 덧셈으로!) 얻어지게 된다.
    <ul>
      <li>코드로도 당연히 너무 간단하다.
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">H</span> <span class="o">+=</span> <span class="n">J_i</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">J_i</span> 
  <span class="n">b</span> <span class="o">+=</span> <span class="n">J_i</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">e_i</span> 
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>그럼 우리가 지금 유도하려는 것은 결국 저 <code class="language-plaintext highlighter-rouge">J_i</code> 와 <code class="language-plaintext highlighter-rouge">e_i</code> 가 무엇인가 하는 것이다.</li>
  <li><code class="language-plaintext highlighter-rouge">J_i</code> 는 <code class="language-plaintext highlighter-rouge">e_i</code>를 미분함으로써 얻어지는 것이므로, 먼저 <code class="language-plaintext highlighter-rouge">e_i</code> 를 정의해야 한다.</li>
  <li><code class="language-plaintext highlighter-rouge">e_i</code>는 다음과 같다.
    <ul>
      <li>우리는 iteratively gauss newton optimization 을 수행할 것이므로</li>
      <li>그 최적조금움직임 해는 rotation 의 tangent space 에서만 정의가 가능하다. 거기만이 vector space이기 때문이다.
        <ul>
          <li>이 말이 이해가 가지 않는다면 예전 포스트를 읽고 오기를 추천한다. <a href="/slam/2023/12/17/nano-lie-theory.html">예전 포스트1</a> 와 <a href="/slam/2024/05/25/reprojection-jacobian-r2live.html">예전 포스트2</a>.</li>
        </ul>
      </li>
      <li>원래의 rotation 공간인 $\mathrm{SO(3)}$ 에서의 residual (은 3 by 3 matrix 이다) 에 $\textbf{Log}(\cdot)$ 연산을 취해주면 3-dim vector가 생성된다.</li>
      <li>그것이 우리의 <code class="language-plaintext highlighter-rouge">e_i</code>가 된다.</li>
      <li>왜 그렇게 하냐고 묻는다면, 다시 요약하자면, Gauss-Newton opt 는 vector space 에서만 수행할 수 있는데, 그것은 $\mathrm{SO(3)}$ 의 tangent space인 $\mathfrak{so(3)}$ 에서 정의된다고 할 수 있다. 그래서 우리는 $\mathrm{SO(3)}$ 에서 distance 를 구하고, 거기에 $\textbf{Log}(\cdot)$ 연산을 취해서, $\mathfrak{so(3)}$ 상에서의 distance 를 정의했다고 할 수 있다.
        <ul>
          <li>ps. 왜 $\textbf{Log}(\cdot)$ 연산을 취해야, $\mathfrak{so(3)}$ 상에서의 distance 가 되는지에 대해서는 여기에서 더 설명하지는 않는다. 그것은 약간 기본이기 때문… 여기서부터는 <a href="https://arxiv.org/abs/1711.02508">Quaternion kinematics for the error-state Kalman filter</a> 이나, 아니면 다른 논문들을 참고 바람.</li>
        </ul>
      </li>
      <li>그리고 $\textbf{Log}(\cdot)$ 는 사실 $\textbf{log}(\cdot)$ 라는 연산에 unskew 를 해주는 과정을 더한 것을, 간단하게 문자로 표현하려고 l을 L로만 바꿔쓴것이다.</li>
    </ul>
  </li>
  <li>그럼 이제까지 tangent space 상에서의 error function (our observation model) 을 정의했으니, 이것의 Jacobian을 구해보자.</li>
  <li>먼저, <code class="language-plaintext highlighter-rouge">e_i</code>의 수식을 코드가 아니라 latex으로 다시 적어보자.
    <ul>
      <li>$e_i = \textbf{Log}(error(\mathbf{R}))$ 이고, 여기서 $error(\mathbf{R})$는 다음과 같이 정의된다. $error(\mathbf{R}) = \mathbf{R}_{\text{measurement}} \ominus \mathbf{R}_{\text{estimated}} = \mathbf{R}_{\text{estimated}}^\top \cdot \mathbf{R}_{\text{measurement}}$
        <ul>
          <li>왜 transpose 의 왼쪽곱이 되는지 까지 여기서 설명하지는 않는다..</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>앞서 우리의 measurement 를 간단히 $\mathbf{R}_{i}$ 라 불렀고 (예: $\mathbf{R}_{1}$, $\mathbf{R}_{2}$, $\mathbf{R}_{3}$), 우리의 (step $t$에서의) 예측값을 $\mathbf{R}^{(t)}$라고 불렀으니, 다시 식을 써보면</li>
  <li>$e_i = \textbf{Log}({\mathbf{R}^{(t)}}^\top \cdot \mathbf{R}_{i})$ 라고 할 수 있다.</li>
  <li>이제 $\mathbf{J}_i$를 논할 수 있다.</li>
  <li>$\mathbf{J}_i$는, 우리의 관심사(최적조금움직임값이 얼마인지)인 tangent space 에서의 value (rotation vector 라고 하자) 인, 이 3-dim vector인 이 값이 조금 움직일 때, 위의 $e_i$가 얼마나 변하는지? 로 정의된다.</li>
  <li>그 최적으로 조금 움직인 rotation vector 를 $\delta \boldsymbol{\theta}$라고 하면, (ps. 조금이라는 의미에서 $\delta$ 를 붙임)</li>
  <li>이 때 (== step $t$에서 조금만 최적으로 더 움직여 본)의, 예측 값은 ${(\mathbf{R}^{(t)} \oplus \delta\boldsymbol{\theta})}$ 가 되고,</li>
  <li>$e_i$는 따라서 $\textbf{Log}({(\mathbf{R}^{(t)} \oplus \delta\boldsymbol{\theta})}^\top \cdot \mathbf{R}_{i})$ 가 되며,
    <ul>
      <li>ps. 이 때 $\oplus$란 matmul 을 의미한다. 하지만 왜 그렇게 쓰나면, $\delta\boldsymbol{\theta}$ 는 vector 이고, rotation matrix 가 살고있는 SO(3) 에 살고있지 않기 때문에, 일반적인 더하기나 곱하기 (i.e., $\cdot$) 기호를 쓰면 논리가 맞지않게 되어 편의상 $\oplus$ 나 $\boxplus$와 같은 기호로 표기하는 풍습이 있다. 실제로는 oplus 가 적혀있다면, rotation vector 에 Exp를 취한 뒤 matmul 을 해준다 라고 이해하면 된다.
        <ul>
          <li>이미 존재하는 다양한 공부자료들로 그 설명을 대신한다.
            <ul>
              <li><a href="https://arxiv.org/pdf/1107.1119">Integrating Generic Sensor Fusion Algorithms with Sound State Representations through Encapsulation of Manifolds</a></li>
              <li><a href="https://ingmec.ual.es/~jlblanco/papers/jlblanco2010geometry3D_techrep.pdf">A tutorial on SE(3) transformation parameterizations and on-manifold optimization</a></li>
              <li><a href="https://xipengwang.github.io/lie-theory/#$\boxplus$-and-$\boxminus$">Lie Theory for State Estimation in Robotics</a></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>$J_i = \frac{\partial e_{i}(\delta \boldsymbol{\theta})}{\partial \delta\boldsymbol{\theta}} = \frac{\partial \textbf{Log}({(\mathbf{R}^{(t)} \oplus \delta\boldsymbol{\theta})}^\top \cdot \mathbf{R}_{i})}{\partial \delta\boldsymbol{\theta}}$ 가 된다.</li>
  <li>이것이 실제로 코드상으로 어떻게 구현이 되는지, symforce 등에 맡겨도 되겠지만…</li>
  <li>우리가 알고 있는 rotation 의 유용한 수식들을 잘 조합해서, 잘 풀어서 deterministic 하게 써내면 (== 코드로 옮기기 쉬운 형태) 우리의 목표 달성이다.</li>
  <li>트릭 1
    <ul>
      <li>${\mathbf{R}^{(t)} \oplus \delta\boldsymbol{\theta}}$ 은</li>
      <li>$\mathbf{R}^{(t)} \cdot \textbf{Exp}(\delta\boldsymbol{\theta})$ 인데,
        <ul>
          <li>여기서, $\delta\boldsymbol{\theta}$ 가 작은 값일 때는 (최적 ‘조금’움직임 값이므로 이렇게 가정할 수 있음)</li>
          <li>$\textbf{Exp}(\delta\boldsymbol{\theta}) \sim \textbf{I} + [\delta\boldsymbol{\theta}]_{\times}$ 로 근사할 수 있다.
            <ul>
              <li>note: $[ … ]_{\times}$ 는 input이 3-dim vector이고, output이 skew symmetric matrix 인 함수를 의미한다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>그러면 다시, $\mathbf{R}^{(t)} \cdot \textbf{Exp}(\delta\boldsymbol{\theta}) = \mathbf{R}^{(t)} \cdot (\textbf{I} + [\delta\boldsymbol{\theta}]_{\times})$ 가 된다.</li>
    </ul>
  </li>
  <li>그러면 $J_i = \frac{\partial \textbf{Log}({(\mathbf{R}^{(t)} \oplus \delta\boldsymbol{\theta})}^\top \cdot \mathbf{R}_{i})}{\partial \delta\boldsymbol{\theta}}$ 는
    <ul>
      <li>$J_i = \frac{\partial \textbf{Log}({ (\mathbf{R}^{(t)} \cdot (\textbf{I} + [\delta\boldsymbol{\theta}]_{\times})) }^\top \cdot \mathbf{R}_{i})}{\partial \delta\boldsymbol{\theta}}$ 와 같이 다시 쓸 수 있다.</li>
    </ul>
  </li>
  <li>트릭 2
    <ul>
      <li>$\textbf{Log}(\cdot)$ 연산자를 $\text{unskew}(\textbf{log}(\cdot))$ 이렇게 나눠서 쓸 수 있다. 여기서 unskew 라는 말은 그냥 편의상 내가 붙인 말이며 (좀 더 코드 친화적으로 설명하기 위해서), 실제로는 스큐시메트릭 행렬을 벡터로 다시 변환하는 연산을 “vee” 연산이라고 흔히 부른다. $\vee$ 기호로 쓴다.</li>
      <li>그러면 $J_i = \frac{\partial \texttt{vee}(\textbf{log}({ (\mathbf{R}^{(t)} \cdot (\textbf{I} + [\delta\boldsymbol{\theta}]_{\times})) }^\top \cdot \mathbf{R}_{i}))}{\partial \delta\boldsymbol{\theta}}$ 와 같이 다시 쓸 수 있다.</li>
    </ul>
  </li>
  <li>트릭 3
    <ul>
      <li>$(AB)^\top = B^\top \cdot A^\top$ 을 이용해서, $\textbf{log}(\cdot)$ 안의 식을 정리하면</li>
      <li>$\left( \mathbf{R}^{(t)} \cdot (\mathbf{I} + [\delta\boldsymbol{\theta}]_\times) \right)^\top = (\mathbf{I} + [\delta\boldsymbol{\theta}]_\times)^\top \cdot (\mathbf{R}^{(t)})^\top$ 가 된다.</li>
    </ul>
  </li>
  <li>트릭 4
    <ul>
      <li>skew-symmetric matrix의 정의를 사용하면, $([\delta\boldsymbol{\theta}]_\times)^\top = -[\delta\boldsymbol{\theta}]_\times$ 이다.</li>
      <li>그리고 $(A + B)^\top = A^\top + B^\top$  이므로</li>
      <li>위의 텀 $\left( \mathbf{R}^{(t)} \cdot (\mathbf{I} + [\delta\boldsymbol{\theta}]_\times) \right)^\top = (\mathbf{I} + [\delta\boldsymbol{\theta}]_\times)^\top \cdot (\mathbf{R}^{(t)})^\top$ 은 $ = (\mathbf{I} - [\delta\boldsymbol{\theta}]_\times) \cdot (\mathbf{R}^{(t)})^\top$ 가 된다.</li>
      <li>이제 그러면 residual 식은 $e_i(\delta\boldsymbol{\theta}) = \textbf{Log}\left( (\mathbf{I} - [\delta\boldsymbol{\theta}]_\times) \cdot (\mathbf{R}^{(t)})^\top \cdot \mathbf{R}_i \right)$ 가 된다.</li>
    </ul>
  </li>
  <li>트릭 5
    <ul>
      <li>여기서 $\mathbf{E} = (\mathbf{R}^{(t)})^\top \cdot \mathbf{R}_i$ 라고 할 때,</li>
      <li>최적화 과정에서 현재 추정치 $\mathbf{R}^{(t)}$는 측정치 $\mathbf{R}_i$에 근접하므로 $\mathbf{E}$는 단위 행렬에 근접한다.
        <ul>
          <li>즉, $\mathbf{E} \sim \mathbf{I}$ 라고 할 수 있다.</li>
        </ul>
      </li>
      <li>따라서:
        <ul>
          <li>$e_i(\delta\boldsymbol{\theta}) = \textbf{Log}\left( (\mathbf{I} - [\delta\boldsymbol{\theta}]_\times) \cdot \mathbf{E} \right) = \texttt{vee}(\textbf{log}\left( (\mathbf{I} - [\delta\boldsymbol{\theta}]_\times) \cdot \mathbf{E} \right)) = \texttt{vee}(\textbf{log}\left( (\mathbf{I} - [\delta\boldsymbol{\theta}]_\times)  \right))$ 로 근사할 수 있다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>트릭 5
    <ul>
      <li>테일러 전개를 이용하자</li>
      <li>$\textbf{log}(\mathbf{I} + \mathbf{A}) = \mathbf{A} - \frac{1}{2}\mathbf{A}^2 + \frac{1}{3}\mathbf{A}^3 - \cdots$ 이다.</li>
      <li>2차 항 이상은 작으므로 무시하면,</li>
      <li>$\textbf{log}\left( (\mathbf{I} - [\delta\boldsymbol{\theta}]_\times)  \right) \sim - [\delta\boldsymbol{\theta}]_\times$ 이다.</li>
    </ul>
  </li>
  <li>결론
    <ul>
      <li>따라서 $J_i = \frac{\partial e_{i}(\delta \boldsymbol{\theta})}{\partial \delta\boldsymbol{\theta}} = \frac{\partial \textbf{Log}({(\mathbf{R}^{(t)} \oplus \delta\boldsymbol{\theta})}^\top \cdot \mathbf{R}_{i})}{\partial \delta\boldsymbol{\theta}} = \frac{\partial \texttt{vee}(\textbf{log}\left( (\mathbf{I} - [\delta\boldsymbol{\theta}]_\times)  \right))}{\partial \delta\boldsymbol{\theta}} = \frac{\partial \texttt{vee}(- [\delta\boldsymbol{\theta}]_\times)}{\partial \delta\boldsymbol{\theta}} = \frac{\partial \texttt{unskew}(- \texttt{skew}(\delta\boldsymbol{\theta}))}{\partial \delta\boldsymbol{\theta}} = \frac{\partial (- \delta\boldsymbol{\theta})}{\partial \delta\boldsymbol{\theta}}  = -\mathbf{I} $ 가 된다.
        <ul>
          <li>ps. $\texttt{vee}$ 와 $\texttt{unskew}$ 는 같은 표현이다. skew 작업을 역으로 해준다는 의미에서 코드에서는 unskew 라고 적는게 좀 더 이해가 쉬워서 그렇게 표현해보았다.  마찬가지로 $[\cdot ]_\times$ 라는 표현보다는 $\texttt{skew}$ 라고 적으면 좀 더 짝이 맞는 듯한 느낌이다 (코드 레벨에서 더 좋은 가독성 보유 가능).</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="구현">구현</h2>
<ul>
  <li>지금까지 e 와 J를 구했다.</li>
  <li>따라서 코드레벨에서는 다음과 같다. 매우 간단하다.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># Initialized at the first step's solution
</span>  <span class="n">R_optimal</span> <span class="o">=</span> <span class="p">...</span> <span class="c1"># e.g., = R1 
</span>
  <span class="c1"># Gauss-newton update 
</span>  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_iters</span><span class="p">):</span>
      <span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
      <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
        
      <span class="c1"># gather measurements 
</span>      <span class="k">for</span> <span class="n">R_i</span> <span class="ow">in</span> <span class="p">[</span><span class="n">R1</span><span class="p">,</span> <span class="n">R2</span><span class="p">,</span> <span class="n">R3</span><span class="p">]:</span>
          <span class="n">e_i</span> <span class="o">=</span> <span class="nf">unskew</span><span class="p">(</span><span class="nf">log</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">R_optimal</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">R_i</span><span class="p">)))</span> 
          <span class="n">J_i</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> 

          <span class="n">H</span> <span class="o">+=</span> <span class="n">J_i</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">J_i</span> 
          <span class="n">b</span> <span class="o">+=</span> <span class="n">J_i</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">e_i</span> 
        
      <span class="c1"># solve the normal equation H * dtheta = -b 
</span>      <span class="n">dtheta_optimal</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">solve</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="o">-</span><span class="n">b</span><span class="p">)</span>
        
      <span class="c1"># check termination condition (convergenceness)
</span>      <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">iter </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">: dtheta: </span><span class="si">{</span><span class="n">dtheta</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">dtheta</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">tolerance</span><span class="p">:</span>
          <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The solution converged. terminate the GN opt at iteration </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
          <span class="k">break</span>
        
      <span class="c1"># update the solution 
</span>      <span class="n">R_optimal</span> <span class="o">=</span> <span class="n">R_optimal</span> <span class="o">@</span> <span class="nc">Exp</span><span class="p">(</span><span class="n">dtheta_optimal</span><span class="p">)</span>

      <span class="c1"># (optional) 업데이트 한 SO(3) 가 정의를 만족하도록 강제 (make det = 1)
</span>      <span class="n">U</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">Vt</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">svd</span><span class="p">(</span><span class="n">R0</span><span class="p">)</span>
      <span class="n">R0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">Vt</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">det</span><span class="p">(</span><span class="n">R0</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
          <span class="n">U</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*=</span> <span class="o">-</span><span class="mi">1</span>
          <span class="n">R0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">Vt</span><span class="p">)</span>

</code></pre></div>    </div>
  </li>
</ul>

<h2 id="결론">결론</h2>
<ul>
  <li>코드에서 마지막에 <code class="language-plaintext highlighter-rouge"># update the solution</code> 부분에서, tangent space에서 얻은 최적조금움직임 값을 원래 rotation에 첨가해주는 것을 “retract” operation 이라고 부른다. 전체 과정은 lift-solve-retract pipeline 이라고 불리기도 한다. <a href="/slam/2023/12/17/nano-lie-theory.html">예전 포스트</a> 에서 언급해두었음.</li>
  <li><a href="https://arxiv.org/abs/1512.02363">2016 T-RO Paper 인 On-Manifold Preintegration for Real-Time Visual-Inertial Odometry</a> 의 본질도 사실 이것이 전부이며… 쫄 필요가 없겠다.</li>
  <li>결론적으로 간단한 measurement 3개 짜리 rotation averaging 이라는 task를 직접 from-scratch 부터 구현해봄으로써, 결론적으로는 최신 visual inertial odometry 방법도 이론적으로는 같다는 이야기를 하고 싶었다.</li>
</ul>

<h2 id="실습">실습</h2>
<ul>
  <li>실습 코드는 여기 (<a href="https://github.com/gisbi-kim/numerical_geometric_programming_lectures/blob/main/python/single_rotation_averaging/main_onmanifold_ganussnewton.py">main_onmanifold_ganussnewton.py</a>) 에 있다. (디펜던시는 유명한 numpy 와 matplotlib 이므로 별도 도커환경 설정은 생략하고 실습해보자.)</li>
  <li>GN 최적화 과정에서 weight 를 고려한 update 예시를 구현해보자. 예를 들어서 3개의 measurement 중 하나를 특히 더 신뢰하고 싶다면?
    <ul>
      <li>이를 더 확장하면 outlier robust rotation averaging 까지 실습해볼 수 있겠다. 심한 outlier 를 포함한, 100개의 measurement 인 경우로 확대해보자.</li>
    </ul>
  </li>
</ul>

  </div>

  <hr style="border:1px foo rgb(193, 198, 200)"><div id="disqus_thread" style="margin-top:25px"></div>
  <script>
    var disqus_config = function () {
      this.page.url = 'http://localhost:4000/slam/2024/08/24/rot-avg1.html';
      this.page.identifier = 'http://localhost:4000/slam/2024/08/24/rot-avg1.html';
    };
    (function () {
      var d = document, s = d.createElement('script');
      s.src = 'https://robotics.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments
      powered by Disqus.</a></noscript></div>

    </section>
    <footer class="condensed">
      <ul class="social about-footer condensed"><a href="https://www.linkedin.com/in/giseop-kim-71683088" target="_blank">
          <li>
            <i class="icon-linkedin-squared"></i>
          </li>
        </a><a href="https://scholar.google.com/citations?user=9mKOLX8AAAAJ&hl=ko&oi=ao" target="_blank">
          <li>
            <i class="ai ai-google-scholar"></i>
          </li>
        </a><a href="https://github.com/gisbi-kim" target="_blank">
          <li>
            <i class="icon-github-circled"></i>
          </li>
        </a><a href="https://bit.ly/giseopkim" target="_blank">
          <li>
            <i class="fa fa-user" style="font-size: 2.09em;"></i>
            <!-- <i> notion</i> -->
            <!-- <i class="fa fa-user"></i> -->
          </li>
        </a><a href="https://youtube.com/channel/UCrmVMJ3KEFbDD9EtnAmDT6g" target="_blank">
          <li>
            <i class="icon-youtube"></i>
          </li>
        </a></ul><p class="about-footer condensed">&copy;
        2024</p><div class="about-footer condensed">
        <p>Dark Mode
          <i class="icon-moon"></i>
          <label class="switch">
            <input type="checkbox" class="dark-mode-toggle">
            <span class="slider round" onclick="toggleDarkMode()"></span>
          </label>
        </p>
      </div>
    </footer>
  </main>
  
  <script type="text/javascript" src="/assets/js/darkmode.js"></script>
  
  <script src="/assets/js/simple-jekyll-search.min.js"></script>
  <script src="/assets/js/search.js"></script>
  
</body>

</html>
